{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import scipy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"Speech dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, labels_file, audio_conf, transform=None, normalize=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file contain audio and transcript path.\n",
    "            labels_file (string): Path to the json file contain label dictionary.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.speech_frame = pd.read_csv(csv_file, header=None)\n",
    "        with open(labels_file, 'r') as f:\n",
    "            self.labels = json.loads(f.read())\n",
    "        self.window = audio_conf['window']\n",
    "        self.window_size = audio_conf['window_size']\n",
    "        self.window_stride = audio_conf['window_stride']\n",
    "        self.sampling_rate = audio_conf['sampling_rate']\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_file = self.speech_frame.iloc[idx, 0]\n",
    "        transcript_file = self.speech_frame.iloc[idx, 1]\n",
    "        \n",
    "        signal, _ = sf.read(wav_file)\n",
    "        signal /= 1 << 31\n",
    "        signal = self.spectrogram(signal)\n",
    "        \n",
    "        with open(transcript_file, 'r') as f:\n",
    "            transcript = f.read().strip()\n",
    "        transcript_idx = []\n",
    "        transcript_idx.append(self.labels['<sos>'])\n",
    "        for char in list(transcript):\n",
    "            if char in self.labels:\n",
    "                transcript_idx.append(self.labels[char])\n",
    "        transcript_idx.append(self.labels['<eos>'])\n",
    "        sample = {'signal': signal, 'transcript': np.array(transcript_idx)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def spectrogram(self, signal):\n",
    "        n_fft = int(self.sampling_rate * self.window_size)\n",
    "        win_length = n_fft\n",
    "        hop_length = int(self.sampling_rate * self.window_stride)\n",
    "        # STFT\n",
    "        D = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length,\n",
    "                        window=self.window, win_length=win_length)\n",
    "        spect, phase = librosa.magphase(D)\n",
    "        # S = log(S+1)\n",
    "        spect = np.log1p(spect)\n",
    "        spect = torch.FloatTensor(spect)\n",
    "        if self.normalize:\n",
    "            mean = spect.mean()\n",
    "            std = spect.std()\n",
    "            spect.add_(-mean)\n",
    "            spect.div_(std)\n",
    "            \n",
    "        return spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(object):\n",
    "    \"\"\"Rescale the audio signal and transcript to a given size.\n",
    "\n",
    "    Args:\n",
    "        signal_size (int): Desired output size of signal.\n",
    "        transcript_size (int): Desired output size of transcript.\n",
    "        labels_file (string): Path to the json file contain label dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, signal_size, transcript_size, labels_file):\n",
    "        assert isinstance(signal_size, (int))\n",
    "        assert isinstance(transcript_size, (int))\n",
    "        self.signal_size = signal_size\n",
    "        self.transcript_size = transcript_size\n",
    "        with open(labels_file, 'r') as f:\n",
    "            self.labels = json.loads(f.read())\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        signal, transcript = sample['signal'], sample['transcript']\n",
    "        signal /= 1 << 31\n",
    "        signal = pad_sequences(signal, \n",
    "                               maxlen=self.signal_size, padding='post', \n",
    "                               truncating='post', value=0.0, dtype='float')\n",
    "        transcript = pad_sequences(transcript.reshape(1, -1), \n",
    "                               maxlen=self.transcript_size, padding='post', \n",
    "                               truncating='post', value=self.labels['pad'], dtype='int')\n",
    "        \n",
    "        return {'signal': signal, 'transcript': transcript}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        signal, transcript = sample['signal'], sample['transcript']\n",
    "\n",
    "        return {'signal': torch.from_numpy(signal),\n",
    "                'transcript': torch.from_numpy(transcript)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conf = {'window': 'hamming',\n",
    "              'window_size' : 0.02,\n",
    "              'window_stride' : 0.01,\n",
    "              'sampling_rate': 16000}\n",
    "\n",
    "speech_dataset = SpeechDataset('/media/ai/SpeechRecognition.EN/codebase/DVD/train_manifest.csv', \n",
    "                               'labels_dict.json',\n",
    "                               audio_conf,\n",
    "                               transform=transforms.Compose([Padding(30000, 50, 'labels_dict.json')]) \n",
    "                              )\n",
    "#transform=transforms.Compose([Padding(30000, 50, 'labels_dict.json')]) \n",
    "# ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (161, 30000) (1, 50)\n",
      "1 (161, 30000) (1, 50)\n",
      "2 (161, 30000) (1, 50)\n",
      "3 (161, 30000) (1, 50)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(speech_dataset)):\n",
    "    sample = speech_dataset[i]\n",
    "\n",
    "    print(i, sample['signal'].shape, sample['transcript'].shape)\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0537,  0.0138, -0.0325,  ..., -0.1277, -0.0642, -0.1804],\n",
       "        [-0.1520,  0.0534,  0.0080,  ..., -0.1494, -0.0947, -0.1738],\n",
       "        [-0.0952, -0.0735, -0.1336,  ..., -0.1885, -0.1509, -0.1731],\n",
       "        ...,\n",
       "        [-0.1911, -0.1919, -0.1918,  ..., -0.1924, -0.1914, -0.1923],\n",
       "        [-0.1916, -0.1917, -0.1926,  ..., -0.1920, -0.1917, -0.1919],\n",
       "        [-0.1923, -0.1918, -0.1924,  ..., -0.1919, -0.1920, -0.1923]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(speech_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 161, 30000]) torch.Size([4, 1, 50])\n",
      "torch.Size([4, 161, 30000]) torch.Size([4, 1, 50])\n",
      "torch.Size([4, 161, 30000]) torch.Size([4, 1, 50])\n",
      "torch.Size([4, 161, 30000]) torch.Size([4, 1, 50])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample in enumerate(dataloader):\n",
    "    print(sample['signal'].size(), sample['transcript'].size())\n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
