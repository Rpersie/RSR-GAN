{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"Speech dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.speech_frame = pd.read_csv(csv_file, header=None)\n",
    "        with open(labels, 'r') as f:\n",
    "            self.labels = json.loads(f.read())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_file = self.speech_frame.iloc[idx, 0]\n",
    "        transcript_file = self.speech_frame.iloc[idx, 1]\n",
    "        \n",
    "        signal, sampling_rate = sf.read(wav_file)\n",
    "        \n",
    "        with open(transcript_file, 'r') as f:\n",
    "            transcript = f.read().strip()\n",
    "        transcript_idx = []\n",
    "        for char in list(transcript):\n",
    "            if char in self.labels:\n",
    "                transcript_idx.append(self.labels[char])\n",
    "        sample = {'signal': signal, 'transcript': np.array(transcript_idx)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(object):\n",
    "    \"\"\"Rescale the audio signal and transcript to a given size.\n",
    "\n",
    "    Args:\n",
    "        signal_size (int): Desired output size of signal.\n",
    "        transcript_size (int): Desired output size of transcript.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, signal_size, transcript_size):\n",
    "        assert isinstance(signal_size, (int))\n",
    "        assert isinstance(transcript_size, (int))\n",
    "        self.signal_size = signal_size\n",
    "        self.transcript_size = transcript_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        signal, transcript = sample['signal'], sample['transcript']\n",
    "        signal = pad_sequences(signal.reshape(1, -1), \n",
    "                               maxlen=self.signal_size, padding='post', truncating='post')\n",
    "        transcript = pad_sequences(transcript.reshape(1, -1), \n",
    "                               maxlen=self.transcript_size, padding='post', truncating='post')\n",
    "        \n",
    "        return {'signal': signal, 'transcript': transcript}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        signal, transcript = sample['signal'], sample['transcript']\n",
    "\n",
    "        return {'signal': torch.from_numpy(signal),\n",
    "                'transcript': torch.from_numpy(transcript)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_dataset = SpeechDataset('/media/ai/SpeechRecognition.EN/codebase/DVD/train_manifest.csv', \n",
    "                               'label_dict.json',\n",
    "                              transform=transforms.Compose([\n",
    "                                               Padding(30000, 100),\n",
    "                                               ToTensor()\n",
    "                                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 30000]), torch.Size([1, 100]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = speech_dataset[0]\n",
    "sample['signal'].size(), sample['transcript'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 30000]) torch.Size([1, 100])\n",
      "1 torch.Size([1, 30000]) torch.Size([1, 100])\n",
      "2 torch.Size([1, 30000]) torch.Size([1, 100])\n",
      "3 torch.Size([1, 30000]) torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(speech_dataset)):\n",
    "    sample = speech_dataset[i]\n",
    "\n",
    "    print(i, sample['signal'].size(), sample['transcript'].size())\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
