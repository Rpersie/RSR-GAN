{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from encoder import Encoder\n",
    "from attention import Attention\n",
    "from decoder import Decoder\n",
    "from generator import Generator\n",
    "\n",
    "from data_loader import SpeechDataset, Padding, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='RSR-GAN training')\n",
    "parser.add_argument('--train-manifest', metavar='DIR',\n",
    "                    help='path to train manifest csv', default='data/train_manifest.csv')\n",
    "parser.add_argument('--val-manifest', metavar='DIR',\n",
    "                    help='path to validation manifest csv', default='data/val_manifest.csv')\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--batch-size', default=20, type=int, help='Batch size for training')\n",
    "parser.add_argument('--num-workers', default=4, type=int, help='Number of workers used in data-loading')\n",
    "parser.add_argument('--labels-path', default='labels_dict.json', help='Contains all characters for transcription')\n",
    "parser.add_argument('--window-size', default=.02, type=float, help='Window size for spectrogram in seconds')\n",
    "parser.add_argument('--window-stride', default=.01, type=float, help='Window stride for spectrogram in seconds')\n",
    "parser.add_argument('--window', default='hamming', help='Window type for spectrogram generation')\n",
    "\n",
    "parser.add_argument('--enc-hid-dim', default=256, type=int, help='Encoder hidden dimension')\n",
    "parser.add_argument('--dec-hid-dim', default=256, type=int, help='Decoder hidden dimension')\n",
    "parser.add_argument('--dec-emb-dim', default=256, type=int, help='Decoder embedding dimension')\n",
    "parser.add_argument('--dropout-rate', default=0.2, type=float, help='Dropout rate')\n",
    "\n",
    "parser.add_argument('--epochs', default=500, type=int, help='Number of training epochs')\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true', help='Use cuda to train model')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-4, type=float, help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--max-norm', default=400, type=int, help='Norm cutoff to prevent explosion of gradients')\n",
    "parser.add_argument('--learning-anneal', default=1.1, type=float, help='Annealing applied to learning rate every epoch')\n",
    "parser.add_argument('--silent', dest='silent', action='store_true', help='Turn off progress tracking per iteration')\n",
    "parser.add_argument('--checkpoint', dest='checkpoint', action='store_true', help='Enables checkpoint saving of model')\n",
    "parser.add_argument('--checkpoint-per-batch', default=0, type=int, help='Save checkpoint per batch. 0 means never save')\n",
    "parser.add_argument('--tensorboard', dest='tensorboard', action='store_true', help='Turn on tensorboard graphing')\n",
    "parser.add_argument('--log-dir', default='visualize/rsrgan_final', help='Location of tensorboard log')\n",
    "parser.add_argument('--log-params', dest='log_params', action='store_true', help='Log parameter values and gradients')\n",
    "parser.add_argument('--id', default='Deepspeech training', help='Identifier for visdom/tensorboard run')\n",
    "parser.add_argument('--save-folder', default='models/', help='Location to save epoch models')\n",
    "parser.add_argument('--model-path', default='models/rsrgan_final.pth',\n",
    "                    help='Location to save best validation model')\n",
    "parser.add_argument('--continue-from', default='', help='Continue from checkpoint model')\n",
    "parser.add_argument('--finetune', dest='finetune', action='store_true',\n",
    "                    help='Finetune the model from checkpoint \"continue_from\"')\n",
    "\n",
    "parser.add_argument('--no-shuffle', dest='no_shuffle', action='store_true',\n",
    "                    help='Turn off shuffling and sample from dataset based on sequence length (smallest to largest)')\n",
    "\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('labels_dict.json', 'r') as f:\n",
    "    labels = json.loads(f.read())\n",
    "    \n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNAL_INPUT_SIZE = 1500 \n",
    "TXT_INPUT_SIZE = 135\n",
    "OUTPUT_DIM = 135\n",
    "\n",
    "audio_conf = {'window': 'hamming',\n",
    "              'window_size' : 0.02,\n",
    "              'window_stride' : 0.01,\n",
    "              'sampling_rate': 16000}\n",
    "\n",
    "train_dataset = SpeechDataset('../../SpeechRecognition.EN/deepspeech.cv/data/cv-valid-dev_manifest.csv', \n",
    "                            'labels_dict.json',\n",
    "                            audio_conf,\n",
    "                            transform=transforms.Compose([Padding(SIGNAL_INPUT_SIZE, OUTPUT_DIM, 'labels_dict.json')]) \n",
    "                              )\n",
    "\n",
    "val_dataset = SpeechDataset('../../SpeechRecognition.EN/deepspeech.cv/data/cv-other-dev_manifest.csv', \n",
    "                            'labels_dict.json',\n",
    "                            audio_conf,\n",
    "                            transform=transforms.Compose([Padding(SIGNAL_INPUT_SIZE, OUTPUT_DIM, 'labels_dict.json')]) \n",
    "                              )\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                            shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4,\n",
    "                            shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GRU = 6\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256 \n",
    "DEC_EMB_DIM = 256\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "encoder = Encoder(SIGNAL_INPUT_SIZE, NUM_GRU, ENC_HID_DIM, DEC_HID_DIM, DROPOUT_RATE,)\n",
    "attention = Attention(enc_hid_dim=ENC_HID_DIM, dec_hid_dim=DEC_HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT_RATE, attention)\n",
    "model = Generator(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (encoder): Encoder(\n",
       "    (rnn_stack): ModuleList(\n",
       "      (0): GRU(1500, 256, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "      (1): GRU(512, 256, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "      (2): GRU(512, 256, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "      (3): GRU(512, 256, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "      (4): GRU(512, 256, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "      (5): GRU(512, 256, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (pool): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(135, 256)\n",
       "    (gru): GRU(768, 256, batch_first=True)\n",
       "    (fc): Linear(in_features=1024, out_features=135, bias=True)\n",
       "    (dropout): Dropout(p=0.2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "pad_idx = labels['pad']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, sample in enumerate(iterator):\n",
    "        \n",
    "        src = sample['signal'].type(torch.FloatTensor).to(device)\n",
    "        trg = sample['transcript'].type(torch.FloatTensor).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        _, _, output = model(src, trg)\n",
    "        print(output.size())\n",
    "        break\n",
    "        \n",
    "        #loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "        \n",
    "        #loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        #optimizer.step()\n",
    "        \n",
    "        #epoch_loss += loss.item()\n",
    "        \n",
    "    #return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 135])\n",
      "torch.Size([4, 1, 135])\n",
      "torch.Size([4, 1, 135])\n",
      "torch.Size([4, 1, 135])\n",
      "torch.Size([4, 1, 135])\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 10\n",
    "SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'rsr_gan.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    #train_loss = \n",
    "    train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    \"\"\"\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print('Epoch: ', epoch+1, '| Train Loss: ', train_loss, '| Train PPL: ', math.exp(train_loss),\n",
    "          '| Val. Loss: ', valid_loss, '| Val. PPL: ', math.exp(valid_loss))\n",
    "    \"\"\"\n",
    "    #print('Train loss - ', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '../../SpeechRecognition.EN/deepspeech.cv/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_audio_len = 0\n",
    "max_txt_len = 0\n",
    "for i in tqdm(range(len(speech_dataset))):\n",
    "    sample = speech_dataset[i]\n",
    "    if sample['signal'].shape[1] > max_audio_len:\n",
    "        max_audio_len = sample['signal'].shape[1]\n",
    "        \n",
    "    if sample['transcript'].shape[0] > max_txt_len:\n",
    "        max_txt_len = sample['transcript'].shape[0]\n",
    "\n",
    "max_audio_len, max_txt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv-valid-dev_manifest\n",
    "train_max_audio_len = 1477\n",
    "train_max_txt_len = 135"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
