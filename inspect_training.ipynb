{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from encoder import Encoder\n",
    "from attention import Attention\n",
    "from decoder import Decoder\n",
    "from generator import Generator\n",
    "\n",
    "from data_loader import SpeechDataset, Padding, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--no-shuffle'], dest='no_shuffle', nargs=0, const=True, default=False, type=None, choices=None, help='Turn off shuffling and sample from dataset based on sequence length (smallest to largest)', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='RSR-GAN training')\n",
    "parser.add_argument('--train-manifest', metavar='DIR',\n",
    "                    help='path to train manifest csv', default='data/train_manifest.csv')\n",
    "parser.add_argument('--val-manifest', metavar='DIR',\n",
    "                    help='path to validation manifest csv', default='data/val_manifest.csv')\n",
    "parser.add_argument('--sample-rate', default=16000, type=int, help='Sample rate')\n",
    "parser.add_argument('--batch-size', default=20, type=int, help='Batch size for training')\n",
    "parser.add_argument('--num-workers', default=4, type=int, help='Number of workers used in data-loading')\n",
    "parser.add_argument('--labels-path', default='labels_dict.json', help='Contains all characters for transcription')\n",
    "parser.add_argument('--window-size', default=.02, type=float, help='Window size for spectrogram in seconds')\n",
    "parser.add_argument('--window-stride', default=.01, type=float, help='Window stride for spectrogram in seconds')\n",
    "parser.add_argument('--window', default='hamming', help='Window type for spectrogram generation')\n",
    "\n",
    "parser.add_argument('--enc-hid-dim', default=256, type=int, help='Encoder hidden dimension')\n",
    "parser.add_argument('--dec-hid-dim', default=256, type=int, help='Decoder hidden dimension')\n",
    "parser.add_argument('--dec-emb-dim', default=256, type=int, help='Decoder embedding dimension')\n",
    "parser.add_argument('--dropout-rate', default=0.2, type=float, help='Dropout rate')\n",
    "\n",
    "parser.add_argument('--epochs', default=500, type=int, help='Number of training epochs')\n",
    "parser.add_argument('--cuda', dest='cuda', action='store_true', help='Use cuda to train model')\n",
    "parser.add_argument('--lr', '--learning-rate', default=3e-4, type=float, help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--max-norm', default=400, type=int, help='Norm cutoff to prevent explosion of gradients')\n",
    "parser.add_argument('--learning-anneal', default=1.1, type=float, help='Annealing applied to learning rate every epoch')\n",
    "parser.add_argument('--silent', dest='silent', action='store_true', help='Turn off progress tracking per iteration')\n",
    "parser.add_argument('--checkpoint', dest='checkpoint', action='store_true', help='Enables checkpoint saving of model')\n",
    "parser.add_argument('--checkpoint-per-batch', default=0, type=int, help='Save checkpoint per batch. 0 means never save')\n",
    "parser.add_argument('--tensorboard', dest='tensorboard', action='store_true', help='Turn on tensorboard graphing')\n",
    "parser.add_argument('--log-dir', default='visualize/rsrgan_final', help='Location of tensorboard log')\n",
    "parser.add_argument('--log-params', dest='log_params', action='store_true', help='Log parameter values and gradients')\n",
    "parser.add_argument('--id', default='Deepspeech training', help='Identifier for visdom/tensorboard run')\n",
    "parser.add_argument('--save-folder', default='models/', help='Location to save epoch models')\n",
    "parser.add_argument('--model-path', default='models/rsrgan_final.pth',\n",
    "                    help='Location to save best validation model')\n",
    "parser.add_argument('--continue-from', default='', help='Continue from checkpoint model')\n",
    "parser.add_argument('--finetune', dest='finetune', action='store_true',\n",
    "                    help='Finetune the model from checkpoint \"continue_from\"')\n",
    "\n",
    "parser.add_argument('--no-shuffle', dest='no_shuffle', action='store_true',\n",
    "                    help='Turn off shuffling and sample from dataset based on sequence length (smallest to largest)')\n",
    "\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-134a3bb7f9d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSEQ_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mINPUT_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mENC_HID_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDEC_HID_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = src.size(0)\n",
    "INPUT_SIZE = src.size(2)\n",
    "OUTPUT_DIM = 28\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256 \n",
    "DEC_EMB_DIM = 256\n",
    "DROPOUT_RATE = 0.2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = Encoder(SEQ_LEN, INPUT_SIZE, ENC_HID_DIM, DEC_HID_DIM, DROPOUT_RATE)\n",
    "attention = Attention(enc_hid_dim=ENC_HID_DIM, dec_hid_dim=DEC_HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT_RATE, attention)\n",
    "model = Generator(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (encoder): Encoder(\n",
       "    (bi_gru1): GRU(241, 256, bidirectional=True)\n",
       "    (bi_gru2): GRU(768, 256, bidirectional=True)\n",
       "    (bi_gru3): GRU(768, 256, bidirectional=True)\n",
       "    (bi_gru4): GRU(768, 256, bidirectional=True)\n",
       "    (bi_gru5): GRU(768, 256, bidirectional=True)\n",
       "    (bi_gru6): GRU(768, 256, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (pool): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.2)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(28, 256)\n",
       "    (gru): GRU(768, 256)\n",
       "    (out): Linear(in_features=1024, out_features=28, bias=True)\n",
       "    (dropout): Dropout(p=0.2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conf = {'window': 'hamming',\n",
    "              'window_size' : 0.02,\n",
    "              'window_stride' : 0.01,\n",
    "              'sampling_rate': 16000}\n",
    "\n",
    "speech_dataset = SpeechDataset('/media/ai/SpeechRecognition.EN/codebase/DVD/train_manifest.csv', \n",
    "                               'labels_dict.json',\n",
    "                               audio_conf,\n",
    "                              )\n",
    "#transform=transforms.Compose([Padding(30000, 50, 'labels_dict.json')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1174/265878 [00:03<12:27, 353.98it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "There aren't any elements to reflect in axis 0 of `array`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-09fc5a505d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_txt_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeech_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_audio_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmax_audio_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ai/explore/RSR-GAN/data_loader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msignal\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ai/explore/RSR-GAN/data_loader.py\u001b[0m in \u001b[0;36mspectrogram\u001b[0;34m(self, signal)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# STFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         D = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length,\n\u001b[0;32m---> 81\u001b[0;31m                         window=self.window, win_length=win_length)\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagphase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# S = log(S+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.5/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# Pad the time series so that frames are centered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_fft\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# Window the time series.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.5/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpad_before\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpad_after\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                     raise ValueError(\"There aren't any elements to reflect\"\n\u001b[0;32m-> 1319\u001b[0;31m                                      \" in axis {} of `array`\".format(axis))\n\u001b[0m\u001b[1;32m   1320\u001b[0m                 \u001b[0;31m# Skip zero padding on empty axes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: There aren't any elements to reflect in axis 0 of `array`"
     ]
    }
   ],
   "source": [
    "max_audio_len = 0\n",
    "max_txt_len = 0\n",
    "for i in tqdm(range(len(speech_dataset))):\n",
    "    sample = speech_dataset[i]\n",
    "    if sample['signal'].shape[1] > max_audio_len:\n",
    "        max_audio_len = sample['signal'].shape[1]\n",
    "        \n",
    "    if sample['transcript'].shape[0] > max_txt_len:\n",
    "        max_txt_len = sample['transcript'].shape[0]\n",
    "\n",
    "max_audio_len, max_txt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
