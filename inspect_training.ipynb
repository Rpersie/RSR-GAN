{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from encoder import Encoder\n",
    "from attention import Attention\n",
    "from decoder import Decoder\n",
    "from generator import Generator\n",
    "\n",
    "from data_loader import SpeechDataset, Padding, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('labels_dict.json', 'r') as f:\n",
    "    labels = json.loads(f.read())\n",
    "    \n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv-invalid_manifest.csv      cv-valid-test_manifest.csv   __pycache__\r\n",
      "cv-other-dev_manifest.csv    cv-valid-train_manifest.csv  test_manifest.csv\r\n",
      "cv-other-test_manifest.csv   data_loader.py\t\t  train_manifest.csv\r\n",
      "cv-other-train_manifest.csv  distributed.py\t\t  utils.py\r\n",
      "cv-valid-dev_manifest.csv    __init__.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../../SpeechRecognition.EN/deepspeech.cv/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNAL_SEQ_LEN = 1100 \n",
    "TXT_SEQ_LEN = 189\n",
    "OUTPUT_DIM = len(labels)\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "audio_conf = {'window': 'hamming',\n",
    "              'window_size' : 0.02,\n",
    "              'window_stride' : 0.01,\n",
    "              'sampling_rate': 16000}\n",
    "\n",
    "train_dataset = SpeechDataset('train_manifest.csv', \n",
    "                            'labels_dict.json',\n",
    "                            audio_conf,\n",
    "                            transform=transforms.Compose([Padding(SIGNAL_SEQ_LEN, TXT_SEQ_LEN, 'labels_dict.json')]) \n",
    "                              )\n",
    "\n",
    "val_dataset = SpeechDataset('val_manifest.csv', \n",
    "                            'labels_dict.json',\n",
    "                            audio_conf,\n",
    "                            transform=transforms.Compose([Padding(SIGNAL_SEQ_LEN, TXT_SEQ_LEN, 'labels_dict.json')]) \n",
    "                              )\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                            shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNAL_FEATURE = 161\n",
    "NUM_GRU = 4\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256 \n",
    "DEC_EMB_DIM = 256\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "encoder = Encoder(seq_len=SIGNAL_SEQ_LEN, input_size=SIGNAL_FEATURE, \n",
    "                  enc_hid_dim=ENC_HID_DIM, num_gru=NUM_GRU, \n",
    "                  dec_hid_dim=DEC_HID_DIM, dropout_rate=DROPOUT_RATE, \n",
    "                  device=device, use_pooling=False)\n",
    "\n",
    "attention = Attention(enc_hid_dim=ENC_HID_DIM, dec_hid_dim=DEC_HID_DIM)\n",
    "\n",
    "decoder = Decoder(output_dim=OUTPUT_DIM, emb_dim=DEC_EMB_DIM, \n",
    "                  enc_hid_dim=ENC_HID_DIM, dec_hid_dim=DEC_HID_DIM,\n",
    "                  dropout_rate=DROPOUT_RATE, attention=attention)\n",
    "\n",
    "model = Generator(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if model.cuda:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "pad_idx = labels['pad']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)#ignore_index=pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, sample in tqdm(enumerate(iterator)):\n",
    "        \n",
    "        src = sample['signal'].type(torch.FloatTensor).to(device)\n",
    "        src = src.permute(0, 2, 1)\n",
    "        trg = sample['transcript'].type(torch.LongTensor).to(device)\n",
    "        trg = trg.view(-1, TXT_SEQ_LEN)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #print('src.size - ', src.size(), ' trg.size - ', trg.size())\n",
    "        #break\n",
    "        #print(trg[:, 0])\n",
    "        _, _, output = model(src, trg)\n",
    "        #print('src.size - ', src.size(), ' trg.size - ', trg.size(), ' output.size - ', output.size())\n",
    "        \n",
    "        loss = criterion(output.view(-1, output.shape[2]), trg.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    #print('in evaluation')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, sample in tqdm(enumerate(iterator)):\n",
    "\n",
    "            src = sample['signal'].type(torch.FloatTensor).to(device)\n",
    "            src = src.permute(0, 2, 1)\n",
    "            trg = sample['transcript'].type(torch.LongTensor).to(device)\n",
    "            trg = trg.view(-1, TXT_SEQ_LEN)\n",
    "\n",
    "            _, _, output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #print('output - ', output)\n",
    "            \n",
    "            loss = criterion(output.view(-1, output.shape[2]), trg.view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "417it [14:53,  2.16s/it]\n",
      "84it [00:38,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model.\n",
      "Epoch:  1 | Train Loss:  2.669001927478708 | Train PPL:  14.42556424612975 | Val. Loss:  7.439688052449908 | Val. PPL:  1702.219135230629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "417it [15:03,  2.15s/it]\n",
      "84it [00:38,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 | Train Loss:  2.5587644617048673 | Train PPL:  12.919844487423243 | Val. Loss:  54.036529132298064 | Val. PPL:  2.9360701203938645e+23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [14:58,  2.18s/it]\n",
      "84it [00:38,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model.\n",
      "Epoch:  3 | Train Loss:  2.5277490495777815 | Train PPL:  12.52528059579386 | Val. Loss:  5.6017361879348755 | Val. PPL:  270.8963263077023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [15:06,  2.19s/it]\n",
      "84it [00:38,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 | Train Loss:  2.5028719267399193 | Train PPL:  12.217531479379172 | Val. Loss:  25.934956414358957 | Val. PPL:  183403854666.54712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [15:07,  2.16s/it]\n",
      "84it [00:38,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5 | Train Loss:  2.4842220564826216 | Train PPL:  11.991787691701834 | Val. Loss:  31.017987069629488 | Val. PPL:  29576080803712.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [15:19,  2.17s/it]\n",
      "84it [00:38,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6 | Train Loss:  2.466608320780509 | Train PPL:  11.782416827210849 | Val. Loss:  9.2996346950531 | Val. PPL:  10934.024225377074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [15:21,  2.22s/it]\n",
      "84it [00:39,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model.\n",
      "Epoch:  7 | Train Loss:  2.4668463202689193 | Train PPL:  11.78522137011461 | Val. Loss:  5.438976759002323 | Val. PPL:  230.20650616882773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [15:22,  2.20s/it]\n",
      "84it [00:38,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model.\n",
      "Epoch:  8 | Train Loss:  2.4495893616756375 | Train PPL:  11.583589076846476 | Val. Loss:  5.104474323136466 | Val. PPL:  164.75743858987065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [15:26,  2.25s/it]\n",
      "84it [00:38,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9 | Train Loss:  2.4487693155412193 | Train PPL:  11.574093893177759 | Val. Loss:  5.109905657314119 | Val. PPL:  165.65472582473487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "417it [15:32,  2.22s/it]\n",
      "84it [00:38,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10 | Train Loss:  2.431105112286209 | Train PPL:  11.371441858850378 | Val. Loss:  7.168887053217206 | Val. PPL:  1298.3987496632476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "207it [07:48,  2.25s/it]"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 25\n",
    "CLIP = 10\n",
    "SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'rsr_gan.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    with torch.cuda.device(1):\n",
    "        train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print('Saved Model.')\n",
    "\n",
    "        print('Epoch: ', epoch+1, '| Train Loss: ', train_loss, '| Train PPL: ', math.exp(train_loss),\n",
    "              '| Val. Loss: ', valid_loss, '| Val. PPL: ', math.exp(valid_loss))\n",
    "    \n",
    "    #print('Train loss - ', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_dataset = SpeechDataset('../../SpeechRecognition.EN/deepspeech.cv.i.dvd/data/train_manifest.csv', \n",
    "                            'labels_dict.json',\n",
    "                            audio_conf \n",
    "                              )\n",
    "seq_len = 0\n",
    "transcript_len = 0\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    sample = train_dataset[i]\n",
    "    if sample['signal'].shape[1] > seq_len:\n",
    "        seq_len = sample['signal'].shape[1]\n",
    "        \n",
    "    if sample['transcript'].shape[0] > transcript_len:\n",
    "        transcript_len = sample['transcript'].shape[0] \n",
    "    \n",
    "seq_len, transcript_len\n",
    "\n",
    "\"\"\"\n",
    "# seq_len = 1100, transcript_len = 189"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
